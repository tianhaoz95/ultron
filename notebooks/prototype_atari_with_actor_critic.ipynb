{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of prototype_atari_with_actor_critic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2397e5a4525145f591c86278600805e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_269a9baf1bfe449dbb225fbc4ab16982",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6bd040c6e74b453e8c63b7d406aeb384",
              "IPY_MODEL_f68f06df75e34d88b1593cca8c4cc06b"
            ]
          }
        },
        "269a9baf1bfe449dbb225fbc4ab16982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bd040c6e74b453e8c63b7d406aeb384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fea4648951114944a48aec2f5e988013",
            "_dom_classes": [],
            "description": "step: 987, reward 549.0/599.0:  14%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 284,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98cb2c413a54404cad780f5bcc1ce8c1"
          }
        },
        "f68f06df75e34d88b1593cca8c4cc06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e7ffc6c71c7b49ad997ace448e620277",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 284/2000 [41:04&lt;4:11:44,  8.80s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d69fcdd37c264723b3b701b7569b370d"
          }
        },
        "fea4648951114944a48aec2f5e988013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98cb2c413a54404cad780f5bcc1ce8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7ffc6c71c7b49ad997ace448e620277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d69fcdd37c264723b3b701b7569b370d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianhaoz95/ultron/blob/dev%2Ftry-atari-game/notebooks/prototype_atari_with_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX_OfbY42iKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay tqdm > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg x11-utils > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM66lPmrmUx1",
        "colab_type": "code",
        "outputId": "456e3902-cbe2-41f1-9dd3-71808ffa6fb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gputil > /dev/null 2>&1\n",
        "!pip install pyglet==1.2.4 > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxSzy33ETUiX",
        "colab_type": "code",
        "outputId": "7d2e9188-4ede-454c-e9c3-cdea50aa2ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import GPUtil\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "  try:\n",
        "    print('ERROR: Not connected to a TPU runtime!')\n",
        "    GPUs = GPUtil.getGPUs()\n",
        "    print('GPU count: ' + str(len(GPUs)))\n",
        "  except:\n",
        "    print('Using CPU')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.2.0-rc4\n",
            "ERROR: Not connected to a TPU runtime!\n",
            "GPU count: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StODOp9qm7As",
        "colab_type": "code",
        "outputId": "211067fc-8fe6-4258-d069-b0c7904d9b27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import gym\n",
        "import threading\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import multiprocessing\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from gym.wrappers import Monitor\n",
        "from tensorflow import keras\n",
        "from os import path\n",
        "from time import sleep\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvT2J4Oa6AsX",
        "colab_type": "code",
        "outputId": "dc6c16ae-3acb-4c78-b8ea-854a17b31392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('using tensorflow', tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using tensorflow 2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR4rdPq23olN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  wrapped_env = Monitor(env, './video', force=True)\n",
        "  return wrapped_env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG-mqsx41fhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipelineArgs():\n",
        "  def __init__(self,\n",
        "               game_name='Acrobot-v1',\n",
        "               mode='train',\n",
        "               max_eps=100,\n",
        "               update_freq=20,\n",
        "               gamma=0.99,\n",
        "               model_path='.',\n",
        "               load_model=False,\n",
        "               lr=0.001):\n",
        "    self.game_name = game_name\n",
        "    self.mode = mode\n",
        "    self.gamma = gamma\n",
        "    self.update_freq = update_freq\n",
        "    self.lr = lr\n",
        "    self.max_eps = max_eps\n",
        "    self.load_model = load_model\n",
        "    self.model_path = model_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz8PHZKXb-db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class History():\n",
        "  def __init__(self, args):\n",
        "    self.history = pd.DataFrame()\n",
        "    self.args = args\n",
        "  \n",
        "  def record(self, eps_reward):\n",
        "    self.eps_rewards.append(eps_reward)\n",
        "\n",
        "  def save(self):\n",
        "    self.history['eps_rewards'] = self.eps_rewards\n",
        "    self.history.to_csv(path.join(self.args.model_path, 'history.csv'))\n",
        "  \n",
        "  def load(self):\n",
        "    if path.exists(\n",
        "        path.join(\n",
        "            self.args.model_path, 'history.csv')) and self.args.load_model:\n",
        "      self.history = pd.DataFrame()\n",
        "      self.history.read_csv(path.join(self.args.model_path, 'history.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to5WdjMOBij4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory():\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "  \n",
        "  def store(self, state, action, reward):\n",
        "    self.states.append(state)\n",
        "    self.actions.append(action)\n",
        "    self.rewards.append(reward)\n",
        "\n",
        "  def clear(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPNOZzhineIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Worker(threading.Thread):\n",
        "  # The cumulative episode played\n",
        "  global_eps = 0\n",
        "  best_score = 0\n",
        "  lock = threading.Lock()\n",
        "\n",
        "  def __init__(self,\n",
        "               args,\n",
        "               action_size,\n",
        "               state_size,\n",
        "               global_model,\n",
        "               opt,\n",
        "               pbar):\n",
        "    super(Worker, self).__init__()\n",
        "    self.args = args\n",
        "    self.game_name = self.args.game_name\n",
        "    self.env = gym.make(self.game_name)\n",
        "    self.action_size = action_size\n",
        "    self.state_size = state_size\n",
        "    self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "    if path.exists(\n",
        "        path.join(self.args.model_path, 'best.h5')) and self.args.load_model:\n",
        "      self.local_model.load_weights(\n",
        "          path.join(self.args.model_path, 'best.h5'))\n",
        "    self.eps_loss = 0\n",
        "    self.opt = opt\n",
        "    self.global_model = global_model\n",
        "    self.pbar = pbar\n",
        "\n",
        "  def compute_loss(self, done, new_state, memory):\n",
        "    if done:\n",
        "      reward_sum = 0\n",
        "    else:\n",
        "      reward_sum = self.local_model(\n",
        "          tf.convert_to_tensor(\n",
        "              new_state[None, :],\n",
        "              dtype=tf.float32))[-1].numpy()[0]\n",
        "    discounted_rewards = []\n",
        "    for reward in memory.rewards[::-1]:\n",
        "      reward_sum = reward + self.args.gamma * reward_sum\n",
        "      discounted_rewards.append(reward_sum)\n",
        "    discounted_rewards.reverse()\n",
        "    past_states = np.array(memory.states)\n",
        "    logits, values = self.local_model(\n",
        "        tf.convert_to_tensor(past_states, dtype=tf.float32))\n",
        "    advantage = tf.convert_to_tensor(\n",
        "        np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n",
        "    # Calculate the loss for value function which mean how off is our\n",
        "    # predicted value from the true value estimated from the discounted\n",
        "    # reward.\n",
        "    value_loss = advantage ** 2\n",
        "    # Calculate the policy loss\n",
        "    policy = tf.nn.softmax(logits)\n",
        "    entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=policy, logits=logits)\n",
        "    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=memory.actions, logits=logits)\n",
        "    policy_loss *= tf.stop_gradient(advantage)\n",
        "    policy_loss -= 0.01 * entropy\n",
        "    # Combine the value and policy loss to be a single trainable\n",
        "    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
        "    return total_loss\n",
        "\n",
        "  def compute_and_apply_gradient(self, done, new_state, mem):\n",
        "    with tf.GradientTape() as tape:\n",
        "      total_loss = self.compute_loss(done, new_state, mem)\n",
        "    self.eps_loss += total_loss\n",
        "    grads = tape.gradient(\n",
        "        total_loss, self.local_model.trainable_weights)\n",
        "    self.opt.apply_gradients(zip(\n",
        "        grads, self.global_model.trainable_weights))\n",
        "    self.local_model.set_weights(self.global_model.get_weights())\n",
        "\n",
        "  def run(self):\n",
        "    # Prepare variables\n",
        "    mem = Memory()\n",
        "    total_steps = 1\n",
        "    # Check the maximum episode of learning is reached\n",
        "    while Worker.global_eps < self.args.max_eps:\n",
        "      current_state = self.env.reset()\n",
        "      mem.clear()\n",
        "      done = False\n",
        "      ep_reward = 0\n",
        "      self.eps_loss = 0\n",
        "      time_count = 0\n",
        "      ep_steps = 0\n",
        "      # Check if the game is over\n",
        "      while not done:\n",
        "        # Get the policy and play the game\n",
        "        logits, _ = self.local_model(\n",
        "            tf.convert_to_tensor(np.array([current_state]), dtype=tf.float32))\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
        "        new_state, reward, done, info = self.env.step(action)\n",
        "        if done:\n",
        "          reward = -1\n",
        "        ep_reward += reward\n",
        "        mem.store(current_state, action, reward)\n",
        "        # If the explore time limit has been reached or\n",
        "        # the game is over, then update the models\n",
        "        if time_count >= self.args.update_freq or done:\n",
        "          self.compute_and_apply_gradient(done, new_state, mem)\n",
        "          mem.clear()\n",
        "          time_count = 0\n",
        "          if done:\n",
        "            with Worker.lock:\n",
        "              if Worker.global_eps == 0:\n",
        "                Worker.best_score = ep_reward\n",
        "              if ep_reward > Worker.best_score:\n",
        "                self.global_model.save_weights(\n",
        "                    path.join(self.args.model_path, 'best.h5'))\n",
        "                Worker.best_score = ep_reward\n",
        "              else:\n",
        "                self.global_model.save_weights(\n",
        "                    path.join(self.args.model_path, 'backup.h5'))\n",
        "              Worker.global_eps += 1\n",
        "              self.pbar.set_description(\n",
        "                  f'step: {ep_steps}, reward {ep_reward}/{Worker.best_score}')\n",
        "              self.pbar.update(1)\n",
        "        ep_steps += 1\n",
        "        time_count += 1\n",
        "        current_state = new_state\n",
        "        total_steps += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnSC4zC4nt9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCriticModel(keras.Model):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super(ActorCriticModel, self).__init__()\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.conv_0 = keras.layers.Conv2D(\n",
        "        filters=3,\n",
        "        activation='relu',\n",
        "        kernel_size=3,\n",
        "        input_shape=(None, 210, 160, 3))\n",
        "    self.flatten  =keras.layers.Flatten()\n",
        "    self.policy_dense_0 = keras.layers.Dense(\n",
        "        128, kernel_initializer='random_uniform')\n",
        "    self.policy_dense_1 = keras.layers.Dense(\n",
        "        64, kernel_initializer='random_uniform')\n",
        "    self.policy_logits = keras.layers.Dense(self.action_size, activation='relu')\n",
        "    self.value_dense = keras.layers.Dense(\n",
        "        128, kernel_initializer='random_uniform', activation='relu')\n",
        "    self.value = keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.conv_0(inputs)\n",
        "    x = self.flatten(x)\n",
        "    p = self.policy_dense_0(x)\n",
        "    p = self.policy_dense_1(p)\n",
        "    logits = self.policy_logits(p)\n",
        "    v = self.value_dense(x)\n",
        "    values = self.value(v)\n",
        "    return logits, values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtTON1xmoJCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineModel():\n",
        "  def __init__(self):\n",
        "    print('not implemented')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVDqVpOqn_e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MasterAgent():\n",
        "  def __init__(self, args):\n",
        "    self.args = args\n",
        "    self.game_name = self.args.game_name\n",
        "    env = gym.make(self.game_name)\n",
        "    self.action_size = env.action_space.n\n",
        "    self.state_size = env.observation_space.shape[0]\n",
        "    self.opt = tf.optimizers.Adam(self.args.lr)\n",
        "    print('state_size: ', self.state_size)\n",
        "    print('action_size: ', self.action_size)\n",
        "    self.global_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "    self.global_model(\n",
        "      tf.convert_to_tensor(\n",
        "      np.random.random((5, 210, 160, 3)),\n",
        "      dtype=tf.float32))\n",
        "    if path.exists(\n",
        "        path.join(self.args.model_path, 'best.h5')) and self.args.load_model:\n",
        "      self.global_model.load_weights(\n",
        "          path.join(self.args.model_path, 'best.h5'))\n",
        "    self.pbar = tqdm(total=self.args.max_eps)\n",
        "  \n",
        "  def train_sync(self):\n",
        "    worker = Worker(self.args, self.action_size, self.state_size,\n",
        "                    self.global_model, self.opt, self.pbar)\n",
        "    worker.run()\n",
        "    self.pbar.close()\n",
        "  \n",
        "  def train_async(self):\n",
        "    Worker.global_eps = 0;\n",
        "    workers = [\n",
        "      Worker(self.args, self.action_size, self.state_size,\n",
        "             self.global_model, self.opt, self.pbar)\n",
        "      for i in range(multiprocessing.cpu_count())\n",
        "    ]\n",
        "    for i, worker in enumerate(workers):\n",
        "      print(\"Starting worker {}\".format(i))\n",
        "      worker.start()\n",
        "    [w.join() for w in workers]\n",
        "    self.pbar.close()\n",
        "    self.play()\n",
        "  \n",
        "  def play(self):\n",
        "    env = wrap_env(gym.make(self.args.game_name))\n",
        "    state = env.reset()\n",
        "    model = self.global_model\n",
        "    if path.exists(\n",
        "        path.join(self.args.model_path, 'best.h5')) and self.args.load_model:\n",
        "      self.global_model.load_weights(\n",
        "          path.join(self.args.model_path, 'best.h5'))\n",
        "    done = False\n",
        "    while not done:\n",
        "      state_input = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
        "      logit, _ = model(state_input)\n",
        "      policy = tf.nn.softmax(logit)\n",
        "      action = np.argmax(policy)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "    env.close()\n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy5tn0pA2-tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gym_sanity_check(args):\n",
        "  print('starting gym environment sanity check')\n",
        "  env = wrap_env(gym.make(args.game_name))\n",
        "  inital_observation = env.reset()\n",
        "  sample_action = env.action_space.sample()\n",
        "  print('observation shape: ', np.array(inital_observation).shape)\n",
        "  print('sample action: ', sample_action)\n",
        "  for _ in range(20):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        observation, _, done, _ = env.step(action)\n",
        "  env.close()\n",
        "  show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_4XwuN1oFkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entry_point(args):\n",
        "  if args.mode == 'sanity':\n",
        "    gym_sanity_check(args)\n",
        "  if args.mode == 'play':\n",
        "    master = MasterAgent(args)\n",
        "    master.play()\n",
        "  if args.mode == 'train_sync':\n",
        "    master = MasterAgent(args)\n",
        "    master.train_sync()\n",
        "  if args.mode == 'train_async':\n",
        "    master = MasterAgent(args)\n",
        "    master.train_async()\n",
        "  print('Hello World')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_oklLOXofGu",
        "colab_type": "code",
        "outputId": "3452c2e3-27b2-483d-e045-f734485ec944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "2397e5a4525145f591c86278600805e4",
            "269a9baf1bfe449dbb225fbc4ab16982",
            "6bd040c6e74b453e8c63b7d406aeb384",
            "f68f06df75e34d88b1593cca8c4cc06b",
            "fea4648951114944a48aec2f5e988013",
            "98cb2c413a54404cad780f5bcc1ce8c1",
            "e7ffc6c71c7b49ad997ace448e620277",
            "d69fcdd37c264723b3b701b7569b370d"
          ]
        }
      },
      "source": [
        "entry_point(\n",
        "    PipelineArgs(\n",
        "        game_name='Berzerk-v0',\n",
        "        update_freq=20,\n",
        "        max_eps=2000,\n",
        "        load_model=True,\n",
        "        mode='train_async'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "state_size:  210\n",
            "action_size:  18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2397e5a4525145f591c86278600805e4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting worker 0\n",
            "Starting worker 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-5:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"<ipython-input-10-c2ac032f848b>\", line 102, in run\n",
            "    self.compute_and_apply_gradient(done, new_state, mem)\n",
            "  File \"<ipython-input-10-c2ac032f848b>\", line 66, in compute_and_apply_gradient\n",
            "    total_loss = self.compute_loss(done, new_state, mem)\n",
            "  File \"<ipython-input-10-c2ac032f848b>\", line 51, in compute_loss\n",
            "    value_loss = advantage ** 2\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 997, in binary_op_wrapper\n",
            "    return func(x, y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n",
            "    return target(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 506, in pow\n",
            "    return gen_math_ops._pow(x, y, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 6569, in _pow\n",
            "    _ops.raise_from_not_ok_status(e, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\n",
            "    six.raise_from(core._status_to_exception(e.code, message), None)\n",
            "  File \"<string>\", line 3, in raise_from\n",
            "tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Pow: Attempted to set tensor for existing mirror. [Op:Pow]\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j16Rl2eF-9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}