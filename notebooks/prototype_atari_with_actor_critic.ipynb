{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of prototype_atari_with_actor_critic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianhaoz95/ultron/blob/dev%2Ftry-atari-game/notebooks/prototype_atari_with_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX_OfbY42iKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg x11-utils > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM66lPmrmUx1",
        "colab_type": "code",
        "outputId": "c8464490-2c82-4582-fa9e-d05210a7eafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StODOp9qm7As",
        "colab_type": "code",
        "outputId": "1908b6c6-37d0-460d-b694-2d831518bccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import gym\n",
        "import threading\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from gym.wrappers import Monitor\n",
        "from tensorflow import keras\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1021'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1021'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR4rdPq23olN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG-mqsx41fhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipelineArgs():\n",
        "  def __init__(self,\n",
        "               game_name,\n",
        "               mode,\n",
        "               lr):\n",
        "    self.game_name = game_name\n",
        "    self.mode = mode\n",
        "    self.lr = lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPNOZzhineIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Worker(threading.Thread):\n",
        "  # The cumulative episode played\n",
        "  global_eps = 0\n",
        "  best_score = 0\n",
        "  lock = threading.Lock()\n",
        "\n",
        "  def __init__(self,\n",
        "               args,\n",
        "               game_name):\n",
        "    super(Worker, self).__init__()\n",
        "    self.args = args\n",
        "    self.game_name = game_name\n",
        "    self.env = gym.make(self.game_name)\n",
        "\n",
        "  def compute_loss(self, done, new_state, memory):\n",
        "    if done:\n",
        "      reward_sum = 0\n",
        "    else:\n",
        "      reward_sum = self.local_model(\n",
        "          tf.convert_to_tensor(\n",
        "              new_state[None, :],\n",
        "              dtype=tf.float32))[-1].numpy()[0]\n",
        "    discounted_rewards = []\n",
        "    for reward in memory.rewards[::-1]:\n",
        "      reward_sum = reward + self.args.gamma * reward_sum\n",
        "      discounted_rewards.append(reward_sum)\n",
        "    discounted_rewards.reverse()\n",
        "    logits, values = self.local_model(\n",
        "        tf.convert_to_tensor(np.vstack(memory.states), dtype=tf.float32))\n",
        "    advantage = tf.convert_to_tensor(\n",
        "        np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n",
        "    # Calculate the loss for value function which mean how off is our\n",
        "    # predicted value from the true value estimated from the discounted\n",
        "    # reward.\n",
        "    value_loss = advantage ** 2\n",
        "    # Calculate the policy loss\n",
        "    policy = tf.nn.softmax(logits)\n",
        "    entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=policy, logits=logits)\n",
        "    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=memory.actions, logits=logits)\n",
        "    policy_loss *= tf.stop_gradient(advantage)\n",
        "    policy_loss -= 0.01 * entropy\n",
        "    # Combine the value and policy loss to be a single trainable\n",
        "    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
        "    return total_loss\n",
        "\n",
        "  def run(self, args):\n",
        "    # Prepare variables\n",
        "    total_steps = 1\n",
        "    eps_steps = 0\n",
        "    # Check the maximum episode of learning is reached\n",
        "    while Worker.global_eps < self.args.max_eps:\n",
        "      current_state = self.env.reset()\n",
        "      done = False\n",
        "      time_count = 0\n",
        "      # Check if the game is over\n",
        "      while not done:\n",
        "        # Get the policy and play the game\n",
        "        logits, _ = self.local_model(\n",
        "            tf.convert_to_tensor(current_state[None, 1]), dtype=tf.float32)\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
        "        new_state, reward, done, _ = self.env.step(action)\n",
        "        # If the game is over, give it a negative reward\n",
        "        if done:\n",
        "          reward = -1\n",
        "        # If the explore time limit has been reached or\n",
        "        # the game is over, then update the models\n",
        "        if time_count >= self.args.update_freq or done:\n",
        "          with tf.GradientTape() as tape:\n",
        "            total_loss = self.compute_loss()\n",
        "          self.eps_loss += total_loss\n",
        "          grads = tape.gradient(\n",
        "              total_loss, self.local_model.trainable_weights)\n",
        "          self.opt.apply_gradients(zip(\n",
        "              grads, self.global_model.trainable_weights))\n",
        "          self.local_model.set_weights(self.global_model.get_weights())\n",
        "          time_count = 0\n",
        "          if done and ep_reward > Worker.best_score:\n",
        "            with Worker.lock:\n",
        "              print('saving best model so far')\n",
        "        else:\n",
        "          eps_steps += 1\n",
        "          time_count += 1\n",
        "          current_state = new_state\n",
        "          total_steps += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnSC4zC4nt9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCriticModel(keras.Model):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super(ActorCriticModel, self).__init__()\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.policy_dense = keras.layers.Dense(100)\n",
        "    self.policy_logits = keras.layers.Dense(self.action_size, activation='relu')\n",
        "    self.value_dense = keras.layers.Dense(100, activation='relu')\n",
        "    self.value = keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    p = self.policy_dense(inputs)\n",
        "    logits = self.policy_logits(p)\n",
        "    v = self.value_dense(inputs)\n",
        "    values = self.value(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtTON1xmoJCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineModel():\n",
        "  def __init__(self):\n",
        "    print('not implemented')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVDqVpOqn_e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MasterAgent():\n",
        "  def __init__(self, args):\n",
        "    self.args = args\n",
        "    self.game_name = args.game_name\n",
        "    env = gym.make(self.game_name)\n",
        "    self.action_size = env.observation_space.shape[0]\n",
        "    self.state_size = env.action_space.n\n",
        "    self.opt = tf.optimizers.Adam(self.args.lr)\n",
        "    self.global_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "  \n",
        "  def train(self):\n",
        "    print('not implemented')\n",
        "  \n",
        "  def play(self):\n",
        "    env = wrap_env(gym.make(self.game_name))\n",
        "    state = env.reset()\n",
        "    model = self.global_model\n",
        "    done = False\n",
        "    try:\n",
        "      while not done:\n",
        "        env.render()\n",
        "        logit, _ = model(\n",
        "            tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
        "        policy = tf.nn.softmax(logit)\n",
        "        action = np.argmax(policy)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"Received Keyboard Interrupt. Shutting down.\")\n",
        "    finally:\n",
        "      env.close()\n",
        "      show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy5tn0pA2-tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gym_sanity_check():\n",
        "  print('starting gym environment sanity check')\n",
        "  env = wrap_env(gym.make('Breakout-ram-v0'))\n",
        "  inital_observation = observation = env.reset()\n",
        "  print('observation size: ', len(inital_observation))\n",
        "  print('sample observation: ', inital_observation)\n",
        "  for _ in range(20):\n",
        "      observation = env.reset()\n",
        "      done = False\n",
        "      while not done:\n",
        "          env.render()\n",
        "          action = env.action_space.sample()\n",
        "          observation, _, done, _ = env.step(action)\n",
        "  env.close()\n",
        "  show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_4XwuN1oFkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entry_point(args):\n",
        "  if args.mode == 'sanity':\n",
        "    gym_sanity_check()\n",
        "  if args.mode == 'play':\n",
        "    master = MasterAgent(args)\n",
        "    master.play()\n",
        "  print('Hello World')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_oklLOXofGu",
        "colab_type": "code",
        "outputId": "e7e2f08a-de9f-4941-e342-8e48beb441f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        }
      },
      "source": [
        "entry_point(\n",
        "    PipelineArgs(\n",
        "        game_name='Breakout-ram-v0', \n",
        "        mode='play',\n",
        "        lr=0.01\n",
        "    )\n",
        ")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-eccae5989223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mgame_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Breakout-ram-v0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'play'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-78-47f231c2e552>\u001b[0m in \u001b[0;36mentry_point\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'play'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasterAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hello World'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-f7a9a4ecb8a7>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         logit, _ = model(\n\u001b[0;32m---> 23\u001b[0;31m             tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Agwqbfloms8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}