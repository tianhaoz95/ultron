{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of prototype_atari_with_actor_critic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianhaoz95/ultron/blob/dev%2Ftry-atari-game/notebooks/prototype_atari_with_actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX_OfbY42iKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay tqdm > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg x11-utils > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM66lPmrmUx1",
        "colab_type": "code",
        "outputId": "55e1374c-83aa-472d-8089-e6150c7c7597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        }
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gputil > /dev/null 2>&1\n",
        "!pip install pyglet > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxSzy33ETUiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import GPUtil\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "  try:\n",
        "    print('ERROR: Not connected to a TPU runtime!')\n",
        "    GPUs = GPUtil.getGPUs()\n",
        "    print('GPU count: ' + str(len(GPUs)))\n",
        "  except:\n",
        "    print('Using CPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StODOp9qm7As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import threading\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import multiprocessing\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from gym.wrappers import Monitor\n",
        "from tensorflow import keras\n",
        "from time import sleep\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvT2J4Oa6AsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('using tensorflow', tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR4rdPq23olN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG-mqsx41fhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PipelineArgs():\n",
        "  def __init__(self,\n",
        "               game_name='Breakout-ram-v0',\n",
        "               mode='train',\n",
        "               max_eps=100,\n",
        "               update_freq=20,\n",
        "               gamma=0.99,\n",
        "               lr=0.001):\n",
        "    self.game_name = game_name\n",
        "    self.mode = mode\n",
        "    self.gamma = gamma\n",
        "    self.update_freq = update_freq\n",
        "    self.lr = lr\n",
        "    self.max_eps = max_eps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to5WdjMOBij4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory():\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "  \n",
        "  def store(self, state, action, reward):\n",
        "    self.states.append(state)\n",
        "    self.actions.append(action)\n",
        "    self.rewards.append(reward)\n",
        "\n",
        "  def clear(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPNOZzhineIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Worker(threading.Thread):\n",
        "  # The cumulative episode played\n",
        "  global_eps = 0\n",
        "  best_score = 0\n",
        "  lock = threading.Lock()\n",
        "\n",
        "  def __init__(self,\n",
        "               args,\n",
        "               action_size,\n",
        "               state_size,\n",
        "               global_model,\n",
        "               opt,\n",
        "               pbar):\n",
        "    super(Worker, self).__init__()\n",
        "    self.args = args\n",
        "    self.game_name = self.args.game_name\n",
        "    self.env = gym.make(self.game_name)\n",
        "    self.action_size = action_size\n",
        "    self.state_size = state_size\n",
        "    self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "    self.eps_loss = 0\n",
        "    self.opt = opt\n",
        "    self.global_model = global_model\n",
        "    self.pbar = pbar\n",
        "\n",
        "  def compute_loss(self, done, new_state, memory):\n",
        "    if done:\n",
        "      reward_sum = 0\n",
        "    else:\n",
        "      reward_sum = self.local_model(\n",
        "          tf.convert_to_tensor(\n",
        "              new_state[None, :],\n",
        "              dtype=tf.float32))[-1].numpy()[0]\n",
        "    discounted_rewards = []\n",
        "    for reward in memory.rewards[::-1]:\n",
        "      reward_sum = reward + self.args.gamma * reward_sum\n",
        "      discounted_rewards.append(reward_sum)\n",
        "    discounted_rewards.reverse()\n",
        "    past_states = np.vstack(memory.states)\n",
        "    logits, values = self.local_model(\n",
        "        tf.convert_to_tensor(past_states, dtype=tf.float32))\n",
        "    advantage = tf.convert_to_tensor(\n",
        "        np.array(discounted_rewards)[:, None], dtype=tf.float32) - values\n",
        "    # Calculate the loss for value function which mean how off is our\n",
        "    # predicted value from the true value estimated from the discounted\n",
        "    # reward.\n",
        "    value_loss = advantage ** 2\n",
        "    # Calculate the policy loss\n",
        "    policy = tf.nn.softmax(logits)\n",
        "    entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        labels=policy, logits=logits)\n",
        "    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=memory.actions, logits=logits)\n",
        "    policy_loss *= tf.stop_gradient(advantage)\n",
        "    policy_loss -= 0.01 * entropy\n",
        "    # Combine the value and policy loss to be a single trainable\n",
        "    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
        "    return total_loss\n",
        "\n",
        "  def compute_and_apply_gradient(self, done, new_state, mem):\n",
        "    with tf.GradientTape() as tape:\n",
        "      total_loss = self.compute_loss(done, new_state, mem)\n",
        "    self.eps_loss += total_loss\n",
        "    grads = tape.gradient(\n",
        "        total_loss, self.local_model.trainable_weights)\n",
        "    self.opt.apply_gradients(zip(\n",
        "        grads, self.global_model.trainable_weights))\n",
        "    self.local_model.set_weights(self.global_model.get_weights())\n",
        "\n",
        "  def run(self):\n",
        "    # Prepare variables\n",
        "    mem = Memory()\n",
        "    total_steps = 1\n",
        "    # Check the maximum episode of learning is reached\n",
        "    while Worker.global_eps < self.args.max_eps:\n",
        "      current_state = self.env.reset()\n",
        "      mem.clear()\n",
        "      new_state, reward, done, info = self.env.step(1)\n",
        "      done = False\n",
        "      ep_reward = 0\n",
        "      self.eps_loss = 0\n",
        "      time_count = 0\n",
        "      ep_steps = 0\n",
        "      # Check if the game is over\n",
        "      while not done:\n",
        "        # Get the policy and play the game\n",
        "        logits, _ = self.local_model(\n",
        "            tf.convert_to_tensor(current_state[None, :], dtype=tf.float32))\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
        "        new_state, reward, done, info = self.env.step(action)\n",
        "        if done:\n",
        "          reward = -1\n",
        "        ep_reward += reward\n",
        "        mem.store(current_state, action, reward)\n",
        "        # If the explore time limit has been reached or\n",
        "        # the game is over, then update the models\n",
        "        if time_count >= self.args.update_freq or done:\n",
        "          self.compute_and_apply_gradient(done, new_state, mem)\n",
        "          mem.clear()\n",
        "          time_count = 0\n",
        "          if done:\n",
        "            with Worker.lock:\n",
        "              if ep_reward > Worker.best_score:\n",
        "                  print('save best model so far')\n",
        "                  self.global_model.save_weights('snapshot.h5')\n",
        "                  Worker.best_score = ep_reward\n",
        "              Worker.global_eps += 1\n",
        "              self.pbar.update(1)\n",
        "        ep_steps += 1\n",
        "        time_count += 1\n",
        "        current_state = new_state\n",
        "        total_steps += 1\n",
        "      print('Episode finished')\n",
        "    print('Worker finished')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnSC4zC4nt9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCriticModel(keras.Model):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super(ActorCriticModel, self).__init__()\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.policy_dense = keras.layers.Dense(40)\n",
        "    self.policy_logits = keras.layers.Dense(self.action_size, activation='relu')\n",
        "    self.value_dense = keras.layers.Dense(40, activation='relu')\n",
        "    self.value = keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    p = self.policy_dense(inputs)\n",
        "    logits = self.policy_logits(p)\n",
        "    v = self.value_dense(inputs)\n",
        "    values = self.value(v)\n",
        "    return logits, values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtTON1xmoJCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaselineModel():\n",
        "  def __init__(self):\n",
        "    print('not implemented')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVDqVpOqn_e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MasterAgent():\n",
        "  def __init__(self, args):\n",
        "    self.args = args\n",
        "    self.game_name = self.args.game_name\n",
        "    env = gym.make(self.game_name)\n",
        "    self.action_size = env.action_space.n\n",
        "    self.state_size = env.observation_space.shape[0]\n",
        "    self.opt = tf.optimizers.Adam(self.args.lr)\n",
        "    print('state_size: ', self.state_size)\n",
        "    print('action_size: ', self.action_size)\n",
        "    self.global_model = ActorCriticModel(self.state_size, self.action_size)\n",
        "    random_input = np.random.random((1, self.state_size))\n",
        "    dummy = tf.convert_to_tensor(random_input, dtype=tf.float32)\n",
        "    self.global_model(dummy)\n",
        "    self.pbar = tqdm(total=self.args.max_eps)\n",
        "    self.pbar.set_description(\"Training progress\")\n",
        "  \n",
        "  def train_sync(self):\n",
        "    worker = Worker(self.args, self.action_size, self.state_size,\n",
        "                    self.global_model, self.opt, self.pbar)\n",
        "    worker.run()\n",
        "    self.play()\n",
        "    self.pbar.close()\n",
        "  \n",
        "  def train_async(self):\n",
        "    workers = [\n",
        "      Worker(self.args, self.action_size, self.state_size,\n",
        "             self.global_model, self.opt, self.pbar)\n",
        "      for i in range(multiprocessing.cpu_count())\n",
        "    ]\n",
        "    for i, worker in enumerate(workers):\n",
        "      print(\"Starting worker {}\".format(i))\n",
        "      worker.start()\n",
        "    [w.join() for w in workers]\n",
        "    self.play()\n",
        "    self.pbar.close()\n",
        "  \n",
        "  def play(self):\n",
        "    env = wrap_env(gym.make(self.args.game_name))\n",
        "    state = env.reset()\n",
        "    model = self.global_model\n",
        "    done = False\n",
        "    while not done:\n",
        "      env.render()\n",
        "      state_input = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
        "      logit, _ = model(state_input)\n",
        "      policy = tf.nn.softmax(logit)\n",
        "      action = np.argmax(policy)\n",
        "      state, reward, done, _ = env.step(action)\n",
        "    env.close()\n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy5tn0pA2-tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gym_sanity_check(args):\n",
        "  print('starting gym environment sanity check')\n",
        "  env = wrap_env(gym.make(args.game_name))\n",
        "  inital_observation = observation = env.reset()\n",
        "  print('observation size: ', len(inital_observation))\n",
        "  print('sample observation: ', inital_observation)\n",
        "  for _ in range(20):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = env.action_space.sample()\n",
        "        observation, _, done, _ = env.step(action)\n",
        "  env.close()\n",
        "  show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_4XwuN1oFkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entry_point(args):\n",
        "  if args.mode == 'sanity':\n",
        "    gym_sanity_check(args)\n",
        "  if args.mode == 'play':\n",
        "    master = MasterAgent(args)\n",
        "    master.play()\n",
        "  if args.mode == 'train_sync':\n",
        "    master = MasterAgent(args)\n",
        "    master.train_sync()\n",
        "  if args.mode == 'train_async':\n",
        "    master = MasterAgent(args)\n",
        "    master.train_async()\n",
        "  print('Hello World')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_oklLOXofGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entry_point(PipelineArgs(mode='train_sync'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j16Rl2eF-9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}